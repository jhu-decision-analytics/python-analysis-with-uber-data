{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Analytics with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterLab Notebooks for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll demonstrate how we can use Python to conduct data analysis faster and, in some cases, more efficiently than we can do in Microsoft Excel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use JupyterLab in Anaconda to write and execute our code for the in-class examples. In line with what [more scientists are doing today](https://www.nature.com/articles/d41586-018-07196-1), we'll use the JupyterLab notebooks to create a \"computational narrative\" to keep a legacy of our research notes and questions, data hypotheses and questions, data analysis, results, visualizations, and conlusions. The Jupyter notebooks allow us to combine plain text, Python code, and other media in a single document that we can also share with others as a document, gist, or through a GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll utilize the Jupyter cell layout to organize, document, and conduct our analysis so that we can continue to use these functions and methods for future analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing our Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to organize documentation and analysis, but our main objective is always the same: make it easy for us (the analyst) and potential collaborators to understand what we are doing or have done. It's tempting to simply write code to get things accomplished quickly, but this leaves little room for improvement (unless you have an extremely good memory) because it makes it difficult to come back to and build on your work, and this makes it difficult for others to understand what you've done to potentially help and further your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Formatting for Headers and Cell Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [Markdown formatting](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) for our notes (the same kind of formatting we used in our GitHub README files) to add headers, text formatting, colors, links, images, etc.\n",
    "\n",
    "By also organizing our notebooks with cells as headers, we can use the JupyterLab [Table of Contents extension](https://github.com/jupyterlab/jupyterlab-toc) to navigate longer notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown and In-cell notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we'll use the markdown notes to type out longer explanations for our notes and potential future questions, and we'll use `# inline notes` to write specific prompts and definitions for our data analysis. As you become more familiar with Python and your analysis, you might find a different approach for documentation to be useful, which is great!\n",
    "\n",
    "We'll aim to write notes for every line of code that we write so that we and others can fully understand our analysis and approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis with Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do in order to conduct data analysis with Python is import the Python libraries that we need to conduct our analysis. We can always update this list as we build out our analysis and explore our data, but it's useful to keep this information at the top of the notebook so that we can keep track of all of the libraries used in this analysis in the event that we need to install them again later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll review some principles to help us with specific analysis examples that we've already covered, but you can find more help in the [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) or in several other free and paid tutorials/stackoverflow questions online and in print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries that we'll use to re-do our data analysis with Excel\n",
    "import pandas as pd # for data analysis\n",
    "import numpy as np # for data analysis\n",
    "from sklearn.linear_model import LinearRegression # for linear regression\n",
    "import matplotlib.pyplot as plt # for initial graphs\n",
    "import plotly.express as px # for interactive graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll work with the uber movement data that we used in our big-picture data analysis in Excel. We can import our data in two ways:\n",
    "1. __From a GitHub Raw link:__ go to the GitHub repository where the data is housed, click on the csv document, and the copy and paste the raw data URL into the space below\n",
    "2. __From a local file:__ Make sure that the uber data file is in the same file as our jupyter notebook, __OR__ make sure that you properly copy the uber excel file's pathname into the space below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import data from a csv on GitHub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable that contains that pathname for our data set\n",
    "\n",
    "# create a data frame with the csv data for us to visualize and manipulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import data from a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file that we moved to the same folder\n",
    "# use path name found from right-clicking on the folder in the left menu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview data (it's the same!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at some big-picture statistics on our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, there are 5 different data types that we'll work with: \n",
    "1. __integers__ (int) - these are numbers that don't have a decimal. \n",
    "2. __floats__ (float) - these are numbers that have a decimal. We can perform math functions with integers & integers and integers & floats\n",
    "3. __strings__ (str) - these are any series of characters, which can be numbers, letters, spaces, special characters, or any combination of those. Strings are delimited with either single quotes ('') or double quotes (\"\")\n",
    "4. __Boolean__ (bool) - data types that are either True or False. Boolean data types are used a lot in Python logic expressions\n",
    "5. __datetime__ (datetime) - for dates and times. Datetime objects allow us to perform calculations with other dates and times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info() tells us what type of data is in our data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some basic statistics for all of the integer columns in our dataset with df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning with Uber Movement Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our data set again: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we cleaned this data in Excel, we wanted to:\n",
    "1. Create a new column that showed our Mean Travel Time in minutes instead of seconds\n",
    "2. Separate out the dates from the unneccessary words/characters in the Date Range column so that we could sort things by date\n",
    "3. Identify the day of the week and day of the week name for each date\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Mean Travel Time into Minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the number in Minutes\n",
    "df_uber_raw[\"Mean Travel Time (Minutes)\"] = round(, #the number you're rounding\n",
    "                                                  ) # the number of digits after the decimal place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our values in the Date Range column\n",
    "df_uber_raw[\"Date Range\"] = df_uber_raw[\"Date Range\"].str.split(, # where you want to split the string\n",
    "                                                                , # the number of columns you want to keep\n",
    "                                                                expand =) # expand the split into new columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Date Range column to datetime\n",
    "df_uber_raw[\"Date Range\"] = pd.to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the name of the week for each date\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the name of the week for each date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis with Uber Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we cleaned our data, we wanted to look at some big-picture data aggregation to look at the average travel time per day, the percentage of time traveled on each week day, and the average travel time per date. We'll aggregate this information with a pandas groupby aggregation.\n",
    "\n",
    "In a pandas groupby, we can perform calculations on a specific group of data within our dataset and then apply these calculations to the current dataframe (as a new column) or create a new dataframe with grouped and aggregated information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we'll use .agg to aggregate values in our dataset based on a chosen group (Day Name) by following the following the following convention: \n",
    "\n",
    "`new_dataframe_name = old_data_frame_name.groupby(\"column_we_want_to_group_values_by\")[\"column_we_want_to_perform_group_calculation_on\"].agg([\"function_we_want_to_perform\"]).reset_index()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data into a new dataframe that only lists the column(s) we identify in the groupby and the aggregating function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also still create pivot tables in Python with the following convention:\n",
    "\n",
    "`new_dataframe = pd.pivot_table(old_dataframe, values = columns_we_want_in_values_fields_of_pivot_table, \n",
    "                                       index = columns_we_want_as_row_fields, \n",
    "                                       columns = columns_we_want_as_column_fields, \n",
    "                                       aggfunc = np.function_we_want_to_perform_on_values).reset_index()`\n",
    "\n",
    "If we want to use more than one column in the values, index, or column fields, then we need to make them into a list. For example, if we wanted to use Day Name and Date Range as our row fields, we would write index = [\"Day Name\", \"Date Range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table with data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualizations with Plotly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some new information to work with, we can create a nice visualization to see what's happening and show this to other people. There are several Python libraries that we can use to make visualizations--we'll be using [Plotly](https://medium.com/plotly/plotly-py-4-0-is-here-offline-only-express-first-displayable-anywhere-fc444e5659ee) and Plotly express to create some interactive visualizations with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a bar chart to show the mean travel time in minutes for each day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart with plotly express\n",
    "\n",
    "weekday_bar = px.bar(, # dataframe of the data we want to plot\n",
    "                     x = , # column on the x axis\n",
    "                     y = , # column on the y axis\n",
    "                     color = , # categorize by different colors\n",
    "                     hover_name = , # values we want to show up when we hover over the chart\n",
    "                    title = , # chart title\n",
    "                    labels = {}) # renaming labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder our week day names so that we see Sunday-Saturday\n",
    "\n",
    "# create a list of week day names in the correct order\n",
    "weekday_name = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "# use Catagorical to sort values into categories\n",
    "df_average_time_day[\"Day Name\"] = pd.Categorical(df_average_time_day[\"Day Name\"],categories=weekday_name)\n",
    "# sort values in our dataframe by the order that we defined\n",
    "df_average_time_day = df_average_time_day.sort_values(\"Day Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at new, ordered dataframe\n",
    "df_average_time_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bubble Chart Example with GDP Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily create timelapse visualizations with dataframes. This [super-engaging TED Talk](https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen?language=en) does a great job of telling a story with the data timelapse visualizations, and we'll learn how to re-create this with Plotly express below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-loaded dataset from Plotly express\n",
    "gapminder = px.data.gapminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bubble chart\n",
    "bubble_gdp = px.scatter(gapminder, x=\"gdpPercap\", \n",
    "                        y=\"lifeExp\", \n",
    "                        animation_frame=\"year\", \n",
    "                        animation_group=\"country\",\n",
    "                        size=\"pop\", \n",
    "                        color=\"continent\", \n",
    "                        hover_name=\"country\",\n",
    "                        log_x=True, \n",
    "                        size_max=55, \n",
    "                        range_x=[100,100000], \n",
    "                        range_y=[25,90],\n",
    "                        title = \"How GDP per Capita, Life Expectancy, and Population Size Change Over Time (1952-2007)\",\n",
    "                       labels = {\"gdpPercap\": \"GDP Per Capita\", \"lifeEx\": \"Life Expectancy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bubble_gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
